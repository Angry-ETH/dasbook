# 概念

## 从“大区块”到“数据可用性”（DA）

区块链的本质，无非是在一定时间内，对一批最新交易达成共识。这批交易组成一个区块，随后被全球各地的节点下载并验证，以确保每笔交易都被正确执行，最终全网就这个区块的合法性达成一致。

要让区块链“变快”，直观的办法是：在相同时间内处理更多交易。然而更多交易意味着更大的区块，随之而来的是一个好消息和一个坏消息。

**坏消息**：节点达成共识的前提，是首先能获得区块数据。而在点对点网络中，将大块数据迅速传播到全球各地的节点，既耗时又容易失败。一些方案试图绕过这个难题，选择牺牲去中心化，仅允许少数节点执行验证工作，但这违背了区块链的初衷。

**好消息**：随着**零知识证明（ZKP）的出现，我们不再需要所有节点接收区块数据**，也能验证区块中的交易是否被正确执行。ZKP 是另一个广阔的话题，此处我们只需了解它的基本原理。

### ZKP 模式

每条链都有一份完整的实时数据，例如所有账户的余额，这些数据被称为状态。区块链接受交易，将旧的数据转换为新的数据，旧的指纹也被新的指纹代替。状态数据非常大，由历史上所有交易计算而来。这个庞大的数据集被压缩成了一个很小的根，相当于人类的指纹。哪怕只改动一个账户余额，这个“指纹”都会发生巨大变化，因此，通过验证新的状态根是否合理，我们就可以确认区块执行的正确性。

![image.png](/zh/ZKP.png)

传统模式下，节点接受区块中的交易，计算出新的状态，生成新的指纹，所有验证者对指纹达成一致。

ZKP 模式下，区块生产者需要多一个计算证明的步骤，用来证明自己的计算确实是正确的。验证节点只需要接受证明、新的指纹即可，无需接收区块数据，这大大降低了参与门槛，让更多节点能加入共识。

但新的问题随之而来。

### 数据可用性

设想：我有 100 ETH，转给朋友 80 ETH。矿工处理该交易，计算出新的状态根（我现在剩 20 ETH），并通过零知识证明向网络证明这个状态变更是正确的。验证者确认无误后，接受这个状态根。

![image.png](/zh/da.png)

但这个矿工并没有对外公布我只剩 20 ETH 的事实，那么下一次我试图转账 10 ETH，其他矿工将无法验证我是否有足够余额。这意味着：只有那个拥有完整数据的矿工能够继续出块，他可以控制整个网络的运转，甚至随时停止整个网络，通过做空市场获利。
为了避免这种情况的出现，除了验证新的状态根计算是否正确的同时，还必须确保这名矿工公布了我新余额的数据。

为了实现这一点，一个简单的办法是所有矿工在验证前首先下载完整数据，但这样一来扩容已失去意义。一种被称为数据可用性的技术的出现解决了这个问题，验证者可以在不下载所有数据的前提下，确认数据已完整存在于网络中。如此一来，确保任何人都可以通过网络下载完整数据。

这正是 L1 扩容、 ZK Rollup 的核心机制，乐观 Rollup 的原理与此略有不同，但底层原理大致相似。

## 什么是 DAS

我们已经知道，如果一个恶意矿工拒绝公布某笔交易执行后的新状态数据，即便他提供了正确的状态根和零知识证明，整个网络依然可能陷入停滞。为了解决这个问题，我们必须确保：验证者无需下载全部数据，也能确认这些数据已被“真实地、完整地”发布到网络中。

这就像有人通过点对点网络上传了一部电影，你不希望花几个小时下载，最后却发现网络中并没有完整的数据。于是，在正式下载之前，你决定先做几个测试：将电影切分为成千上万个数据块，并给每个块编上编号，之后从网络中试着下载一块随机块。如果能成功，我们的信心就多了一份。我们只需要多下载几个随机块，信心就会不断增加，直到我们确信：电影被完整地上传到了网络。

这就是 DAS 的核心原理： 通过对数据块的随机抽样，概率性地确认数据“可用”。但事情并没有这么简单。
你或许有过这样的经历：某个下载软件在经历了数小时后，进度条卡在 99.9%，最后一块数据始终无法下载。你一次次尝试，却迟迟等不到那“最后一块”。最终才意识到：这部电影可能根本就不完整。而即使你之前抽样了几十甚至上百次，也极有可能恰好错过了那缺失的一块。除非你正好抽到了它，否则你永远不会知道它缺不缺。
换句话说，我们只能通过简单的抽样来确保数据大概率是可以被下载的，但对完整性并没有确定性的保障。对于电影来说只是浪费时间，但应用到区块链上则非常致命，可能导致出块停滞、交易隐藏甚至网络攻击等严重后果。
换一种思路，既然无法通过采样确认数据 100% 完整，就退一步，追求一个“足够好”的目标：确保网络中至少有 50% 的数据存在。这在操作上容易多了：

![image.png](/zh/das.png)

- 抽一个数据块，有 50% 的概率能抽中存在的数据；
- 抽两次未命中概率是 0.5 × 0.5；
- 抽样次数越多，未命中的可能性迅速降低；
- 当你抽了 7 次后，数据可用的置信度就非常接近 100%。

更重要的是，这一切与数据的大小无关，无论区块是 1MB 还是 1GB，采样只需要很小带宽。

为什么我们只需要 50% 的数据？因为原始数据经过编码，即便只拿到任意一半的数据块，也能将完整的数据还原出来。这正是 DAS 所带来的奇迹：用概率保障数据可用性，在极低资源消耗的前提下，实现高效安全的扩容。